# Evaluating the Sustainability and Interpretability of CLIP-based Image Retrieval

The rapid advancement of multimodal AI models, particularly those that bridge vision and language, has unlocked powerful new capabilities in information retrieval, content generation, and human-AI interaction. Among these, CLIP (Contrastive Languageâ€“Image Pretraining) has emerged as a state-of-the-art model capable of aligning images and text in a shared semantic space, enabling zero-shot classification, cross-modal retrieval, and rich semantic understanding without task-specific fine-tuning. This has spurred widespread adoption across academic, industrial, and open-source communities. However, as these models grow in complexity and are deployed at scale, two critical challenges emerge: *interpretability* and *sustainability*.

Despite their impressive performance, models like CLIP operate largely as black boxes, making it difficult to understand how and why a particular image-text match is made. This lack of interpretability poses serious concerns for trust, transparency, and accountability, especially in high-stakes domains such as healthcare, legal reasoning, and surveillance. To build confidence in multimodal AI systems, it is crucial to develop tools that not only perform well but also explain their behavior in a human-understandable way. In this work, we adopt recent gradient-based transformer interpretability methods to produce fine-grained relevance maps over both text and image domains, shedding light on the internal mechanisms of CLIP's retrieval process.

In parallel, another dimension that has received growing attention is the *environmental cost* of AI. The carbon footprint of training and deploying large-scale models is increasingly scrutinized in light of global efforts toward sustainability and responsible innovation. While much attention has been paid to the emissions from training massive foundation models, less focus has been given to the inference-time emissions of real-world AI applications. Yet, inference workloads, particularly in retrieval systems used repeatedly at scale, can contribute significantly to operational emissions. Understanding the energy and carbon cost of inference at varying scales and workloads is essential for developing greener AI systems.

In this project, we bring together these two perspectives, interpretability and sustainability, through the lens of a practical multimodal retrieval task using CLIP. Specifically, we:

1. Build an interpretable image-to-text retrieval pipeline based on the Flickr8k dataset, applying transformer-based relevance attribution to visualize how CLIP maps semantic information across modalities.
2. Track carbon emissions and runtime performance using [CodeCarbon](https://mlco2.github.io/codecarbon/), evaluating how these metrics scale with dataset size during both encoding and retrieval phases.
3. Conduct controlled experiments to explore the trade-offs between model performance, transparency, and environmental cost.

By combining interpretability and emissions analysis in a single framework, we aim to offer a more holistic view of what it means to deploy responsible multimodal AI. We envision this work as a step toward tools that not only perform well but also *explain themselves* and do so *efficiently*. Our methodology and findings may inform future research in energy-aware model deployment, transparent retrieval systems, and low-impact AI pipelines.
